{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6633fbe3",
   "metadata": {},
   "source": [
    "# Installing the OpenAI Library\n",
    "### pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a113ddf",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f311b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-7wRvlepFmcEke8EnEeI6iFaXBzF9b', 'object': 'chat.completion', 'created': 1694164745, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'This is a test!'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 5, 'total_tokens': 18}}\n"
     ]
    }
   ],
   "source": [
    "#Importing the OpenAi library\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "YOUR_OPENAI_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your actual API key\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer 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\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74730b",
   "metadata": {},
   "source": [
    "# Checking the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27ccfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Computer Science?</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happened in World War II?</td>\n",
       "      <td>WW2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me everything about Easter and on what da...</td>\n",
       "      <td>Easter?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onChange={e =&gt; {\\n  const inputAmount = {momen...</td>\n",
       "      <td>onChange={e =&gt; {\\n  const inputAmount = {momen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Give me a detailed guide on the best practices...</td>\n",
       "      <td>WPguide UX SEO optimization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0                          What is Computer Science?   \n",
       "1                     What happened in World War II?   \n",
       "2  Tell me everything about Easter and on what da...   \n",
       "3  onChange={e => {\\n  const inputAmount = {momen...   \n",
       "4  Give me a detailed guide on the best practices...   \n",
       "\n",
       "                                          completion  \n",
       "0                                   Computer Science  \n",
       "1                                                WW2  \n",
       "2                                            Easter?  \n",
       "3  onChange={e => {\\n  const inputAmount = {momen...  \n",
       "4                        WPguide UX SEO optimization  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file into a variable\n",
    "with open('C:\\\\Users\\\\HP EliteBook\\\\Desktop\\\\Fine Tuning- GPT 3.5 Turbo\\\\prompts_for_finetuning.json', 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a pandas DataFrame for easier manipulation (optional)\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8635a1",
   "metadata": {},
   "source": [
    "# Printing the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37625222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt', 'completion'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316262b",
   "metadata": {},
   "source": [
    "# Printing the Prompt Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a608b473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Computer Science?\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"prompt\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b067133",
   "metadata": {},
   "source": [
    "# Printing the Completion Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193f3ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067a705",
   "metadata": {},
   "source": [
    "# Checking the number of examples in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26f7582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 101\n",
      "First example:\n",
      "Prompt: What is Computer Science?\n",
      "Completion: Computer Science\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict \n",
    "\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "\n",
    "first_example = dataset[0]\n",
    "print(\"Prompt:\", first_example[\"prompt\"])\n",
    "print(\"Completion:\", first_example[\"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf17f41",
   "metadata": {},
   "source": [
    "# Checking for Errors in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc47c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Format error checks for the provided dataset structure\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:  # Adjusted to access the first element of dataset\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    # Check for existence of \"prompt\" and \"completion\" keys\n",
    "    if \"prompt\" not in ex or \"completion\" not in ex:\n",
    "        format_errors[\"missing_key\"] += 1\n",
    "        continue\n",
    "        \n",
    "    # Check if \"prompt\" and \"completion\" are non-empty strings\n",
    "    if not ex[\"prompt\"] or not isinstance(ex[\"prompt\"], str):\n",
    "        format_errors[\"invalid_prompt\"] += 1\n",
    "        \n",
    "    if not ex[\"completion\"] or not isinstance(ex[\"completion\"], str):\n",
    "        format_errors[\"invalid_completion\"] += 1\n",
    "\n",
    "# Displaying the format errors\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6683670",
   "metadata": {},
   "source": [
    "# Distribution Calculations for every key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a506e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Distribution of prompt and completion tokens:\n",
      "min / max: 7, 2779\n",
      "mean / median: 218.05940594059405, 52.0\n",
      "p5 / p95: 19.0, 674.0\n",
      "\n",
      "#### Distribution of completion tokens:\n",
      "min / max: 2, 227\n",
      "mean / median: 31.099009900990097, 10.0\n",
      "p5 / p95: 4.0, 93.0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# Load the encoding (adjust the model name if needed)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_prompt_and_completion(data_entry):\n",
    "    num_tokens = 0\n",
    "    # Counting tokens for the \"prompt\"\n",
    "    num_tokens += len(encoding.encode(data_entry[\"prompt\"]))\n",
    "    # Counting tokens for the \"completion\"\n",
    "    num_tokens += len(encoding.encode(data_entry[\"completion\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def num_completion_tokens_from_data_entry(data_entry):\n",
    "    return len(encoding.encode(data_entry[\"completion\"]))\n",
    "\n",
    "# Use the dataset to compute token counts for each entry\n",
    "prompt_and_completion_token_counts = [num_tokens_from_prompt_and_completion(entry) for entry in dataset]\n",
    "completion_token_counts = [num_completion_tokens_from_data_entry(entry) for entry in dataset]\n",
    "\n",
    "# Print the distribution for the token counts\n",
    "print_distribution(prompt_and_completion_token_counts, \"prompt and completion tokens\")\n",
    "print_distribution(completion_token_counts, \"completion tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2f696",
   "metadata": {},
   "source": [
    "# Distributions for every key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8916361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Distribution of num_tokens_per_prompt:\n",
      "min / max: 5, 2578\n",
      "mean / median: 186.96039603960395, 40.0\n",
      "p5 / p95: 14.0, 555.0\n",
      "\n",
      "#### Distribution of num_tokens_per_completion:\n",
      "min / max: 2, 227\n",
      "mean / median: 31.099009900990097, 10.0\n",
      "p5 / p95: 4.0, 93.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 7, 2779\n",
      "mean / median: 218.05940594059405, 52.0\n",
      "p5 / p95: 19.0, 674.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Token counts\n",
    "prompt_lens = []\n",
    "completion_lens = []\n",
    "total_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    prompt_len = len(encoding.encode(ex[\"prompt\"]))\n",
    "    completion_len = len(encoding.encode(ex[\"completion\"]))\n",
    "    \n",
    "    prompt_lens.append(prompt_len)\n",
    "    completion_lens.append(completion_len)\n",
    "    total_lens.append(prompt_len + completion_len)\n",
    "    \n",
    "print_distribution(prompt_lens, \"num_tokens_per_prompt\")\n",
    "print_distribution(completion_lens, \"num_tokens_per_completion\")\n",
    "print_distribution(total_lens, \"num_total_tokens_per_example\")\n",
    "\n",
    "# Check for examples that may exceed the 4096 token limit\n",
    "n_too_long = sum(l > 4096 for l in total_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390bdfb0",
   "metadata": {},
   "source": [
    "# Token Count and Pricing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f183303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~22024 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~66072 tokens\n"
     ]
    }
   ],
   "source": [
    "# Constants for Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "# Adjustments based on the constants\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "# Replacing convo_lens with total_lens for token counts\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in total_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2e8fa",
   "metadata": {},
   "source": [
    "# Converting JSON to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5181f3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prompts_for_finetuning.jsonl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert JSON to JSONL\n",
    "with open(\"prompts_for_finetuning.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "jsonl_filename = \"prompts_for_finetuning.jsonl\"\n",
    "with open(jsonl_filename, \"w\") as jsonl_file:\n",
    "    for entry in data:\n",
    "        jsonl_file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "jsonl_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bc6867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file id file-ph4aALw1UC0z3a37AAF7tYdW\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "Waiting for file to process...\n",
      "File processed\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Waiting for fine-tuning to complete...\n",
      "Fine-tuning complete\n",
      "Fine-tuned model info {\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-70pKkeCoW2X5fRSA3PXCxETt\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1694167707,\n",
      "  \"finished_at\": 1694168284,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::7wSqrPKq\",\n",
      "  \"organization_id\": \"org-SQK2FYgHoVOFtdnRBfdMzGRx\",\n",
      "  \"result_files\": [\n",
      "    \"file-nKgiJxpqzpqyLT574w4daYtC\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-ph4aALw1UC0z3a37AAF7tYdW\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 71829,\n",
      "  \"error\": null\n",
      "}\n",
      "Model id ft:gpt-3.5-turbo-0613:personal::7wSqrPKq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Change the filename to match the name of your dataset file\n",
    "file_name = \"prompts_for_finetuning.jsonl\"\n",
    "\n",
    "file_upload = openai.File.create(file=open(file_name, \"rb\"), purpose=\"fine-tune\")\n",
    "print(\"Uploaded file id\", file_upload.id)\n",
    "\n",
    "while True:\n",
    "    print(\"Waiting for file to process...\")\n",
    "    file_handle = openai.File.retrieve(id=file_upload.id)\n",
    "    if len(file_handle) and file_handle.status == \"processed\":\n",
    "        print(\"File processed\")\n",
    "        break\n",
    "    time.sleep(3)\n",
    "\n",
    "# Start the fine-tuning job\n",
    "job = openai.FineTuningJob.create(training_file=file_upload.id, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "while True:\n",
    "    print(\"Waiting for fine-tuning to complete...\")\n",
    "    job_handle = openai.FineTuningJob.retrieve(id=job.id)\n",
    "    if job_handle.status == \"succeeded\":\n",
    "        print(\"Fine-tuning complete\")\n",
    "        print(\"Fine-tuned model info\", job_handle)\n",
    "        print(\"Model id\", job_handle.fine_tuned_model)\n",
    "        break\n",
    "    time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e6209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
